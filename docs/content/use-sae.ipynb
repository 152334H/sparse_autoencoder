{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Trained SAE\n",
    "\n",
    "This demo shows how to use a trained SAE for mechanistic interpretability research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab  # noqa: F401 # type: ignore\n",
    "\n",
    "    in_colab = True\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "\n",
    "#  Install if in Colab\n",
    "if in_colab:\n",
    "    %pip install sparse_autoencoder transformer_lens transformers wandb\n",
    "\n",
    "# Otherwise enable hot reloading in dev mode\n",
    "if not in_colab:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparse_autoencoder import (\n",
    "    ActivationResamplerHyperparameters,\n",
    "    AutoencoderHyperparameters,\n",
    "    Hyperparameters,\n",
    "    LossHyperparameters,\n",
    "    Method,\n",
    "    OptimizerHyperparameters,\n",
    "    Parameter,\n",
    "    PipelineHyperparameters,\n",
    "    SourceDataHyperparameters,\n",
    "    SourceModelHyperparameters,\n",
    "    SweepConfig,\n",
    "    sweep,\n",
    "    SparseAutoencoder,\n",
    "    TensorActivationStore,\n",
    "    Axis,\n",
    "    store_activations_hook,\n",
    ")\n",
    "from functools import partial\n",
    "import os\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import test_prompt\n",
    "from einops import einsum\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "from einops import rearrange\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2 = HookedTransformer.from_pretrained(\"gpt2\")\n",
    "tokenizer: PreTrainedTokenizerBase = gpt2.tokenizer # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact silvery-sweep-7_final:v0, 216.18MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.6\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder(\n",
       "  (pre_encoder_bias): TiedBias(position=pre_encoder)\n",
       "  (encoder): LinearEncoder(\n",
       "    input_features=768, learnt_features=3072, n_components=12\n",
       "    (activation_function): ReLU()\n",
       "  )\n",
       "  (decoder): UnitNormDecoder(learnt_features=3072, decoded_features=768, n_components=12)\n",
       "  (post_decoder_bias): TiedBias(position=post_decoder)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae = SparseAutoencoder.load_from_wandb(wandb_artifact_name=\"alan-cooney/sparse-autoencoder/silvery-sweep-7_final:v0\")\n",
    "\n",
    "sae_cache_names = [f\"blocks.{layer}.hook_mlp_out\" for layer in range(12)]\n",
    "\n",
    "sae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a function to get SAE learned activations from a forward pass of the source and\n",
    "sae models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_sae_activations(\n",
    "    sparse_autoencoder: SparseAutoencoder,\n",
    "    source_model: HookedTransformer,\n",
    "    input_ids: Int[Tensor, Axis.names(Axis.BATCH, Axis.POSITION)],\n",
    "    cache_names: list[str],\n",
    ") -> Float[Tensor, Axis.names(Axis.POSITION, Axis.COMPONENT, Axis.LEARNT_FEATURE)]:\n",
    "    \"\"\"Forward pass that gets the hidden SAE activations.\"\"\"\n",
    "    # Create the store for source model activations\n",
    "    number_activations: int = input_ids.numel()\n",
    "    store = TensorActivationStore(\n",
    "        number_activations,\n",
    "        n_neurons=sparse_autoencoder.config.n_input_features,\n",
    "        n_components=sparse_autoencoder.config.n_components or 1,\n",
    "    )\n",
    "\n",
    "    # Add the hook to the model (will automatically store the activations every time the model\n",
    "    # runs)\n",
    "    source_model.remove_all_hook_fns()\n",
    "    for component_idx, cache_name in enumerate(cache_names):\n",
    "        hook = partial(store_activations_hook, store=store, component_idx=component_idx)\n",
    "        source_model.add_hook(cache_name, hook)\n",
    "\n",
    "    # Source model forward pass (to fill the store)\n",
    "    source_model(input_ids)\n",
    "\n",
    "    # SAE forward pass (to get SAE hidden activations)\n",
    "    sae_res = sparse_autoencoder.forward(store[:])\n",
    "\n",
    "    # Remove the hook\n",
    "    source_model.remove_all_hook_fns()\n",
    "\n",
    "    return sae_res.learned_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Analysis: Identifying MLP enrichment of \"Michael Jordan\" with facts about him"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction - testing the model's capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[] found that MLP layers enrich the subject \"Michael Jordan\" with lots of facts that the model knows\n",
    "about michael Jordan. It was theorised that these facts are stored in MLP layers, but due to\n",
    "polysemanticity it was not possible to identify specific neurons that were involved in this process\n",
    "(i.e. we can't find a \"basketball\" or \"man\" MLP neuron that fires on the Jordan token). In this\n",
    "example we'll use a trained SAE to try and locate specific neurons that are involved in this task.\n",
    "\n",
    "Here's a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Michael', ' Jordan', ' plays', ' the', ' sport', ' of']\n",
      "Tokenized answer: [' basketball']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.17</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50.01</span><span style=\"font-weight: bold\">% Token: | basketball|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m16.17\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m50.01\u001b[0m\u001b[1m% Token: | basketball|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 16.17 Prob: 50.01% Token: | basketball|\n",
      "Top 1th token. Logit: 13.77 Prob:  4.53% Token: | football|\n",
      "Top 2th token. Logit: 13.72 Prob:  4.30% Token: | golf|\n",
      "Top 3th token. Logit: 13.08 Prob:  2.27% Token: | tennis|\n",
      "Top 4th token. Logit: 12.90 Prob:  1.90% Token: | soccer|\n",
      "Top 5th token. Logit: 12.89 Prob:  1.87% Token: | hockey|\n",
      "Top 6th token. Logit: 12.55 Prob:  1.33% Token: | boxing|\n",
      "Top 7th token. Logit: 12.38 Prob:  1.13% Token: | sports|\n",
      "Top 8th token. Logit: 12.24 Prob:  0.98% Token: | the|\n",
      "Top 9th token. Logit: 12.16 Prob:  0.90% Token: | baseball|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' basketball'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' basketball'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prompt(\"Michael Jordan plays the sport of\", answer=\"basketball\", model=gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Michael', ' Jordan', ' is', ' a']\n",
      "Tokenized answer: [' man']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.18</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.11</span><span style=\"font-weight: bold\">% Token: | man|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.18\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m3.11\u001b[0m\u001b[1m% Token: | man|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.18 Prob:  3.11% Token: | man|\n",
      "Top 1th token. Logit: 13.14 Prob:  3.00% Token: | former|\n",
      "Top 2th token. Logit: 12.95 Prob:  2.47% Token: | great|\n",
      "Top 3th token. Logit: 12.64 Prob:  1.82% Token: | big|\n",
      "Top 4th token. Logit: 12.27 Prob:  1.25% Token: | very|\n",
      "Top 5th token. Logit: 12.14 Prob:  1.10% Token: | basketball|\n",
      "Top 6th token. Logit: 12.07 Prob:  1.03% Token: | good|\n",
      "Top 7th token. Logit: 11.94 Prob:  0.90% Token: | legend|\n",
      "Top 8th token. Logit: 11.85 Prob:  0.82% Token: | player|\n",
      "Top 9th token. Logit: 11.84 Prob:  0.82% Token: | star|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' man'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' man'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prompt(\"Michael Jordan is a\", answer=\"man\", model=gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Monosemantic Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple way to do this is some forward passes of the model (with and without the training data),\n",
    "storing the activations of the MLP output layers. We can then run these outputs through the SAE to\n",
    "get SAE hidden layer outputs. Finally we can find which neurons activate for a given proportion of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Michael Jordan\"\n",
    "input_ids: Int[Tensor, Axis.names(Axis.BATCH, Axis.POSITION)] = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct logit attribution (DLA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_sae_activation_cache = forward_pass_sae_activations(sae, gpt2, input_ids, sae_cache_names)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cache here is of shape [component x sae_hidden]. To convert each neuron into residual directions\n",
    "we want a [component x sae_hidden x d_model] tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 3072, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_residual_directions: Float[\n",
    "    Tensor, Axis.names(Axis.COMPONENT, Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)\n",
    "] = einsum(\n",
    "    last_token_sae_activation_cache,\n",
    "    sae.decoder.weight,\n",
    "    (\n",
    "        f\"{Axis.COMPONENT} {Axis.LEARNT_FEATURE} ,\"\n",
    "        f\"{Axis.COMPONENT} {Axis.INPUT_OUTPUT_FEATURE} {Axis.LEARNT_FEATURE} \"\n",
    "        f\"-> {Axis.COMPONENT} {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "neuron_residual_directions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 3072, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_res, cache = gpt2.run_with_cache(input_ids)\n",
    "\n",
    "ln_final_scale = cache[\"ln_final.hook_scale\"][:, -1, :].to(device=\"cpu\")\n",
    "neuron_residual_directions_scaled = neuron_residual_directions / ln_final_scale\n",
    "neuron_residual_directions_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want the residual of the token \"basketball\" (by unembedding it), and thb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basketball_residual_direction = gpt2.tokens_to_residual_directions(\"basketball\").to(\"cpu\")\n",
    "basketball_residual_direction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 3072])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dla = einsum(neuron_residual_directions_scaled, basketball_residual_direction,  (\n",
    "        f\"{Axis.COMPONENT} {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} ,\"\n",
    "        f\"{Axis.INPUT_OUTPUT_FEATURE} \"\n",
    "        f\"-> {Axis.COMPONENT} {Axis.LEARNT_FEATURE}\"\n",
    "    ))\n",
    "dla.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dla = dla.sum(dim=1)\n",
    "layer_dla_df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
