{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A sparse autoencoder for mechanistic interpretability research.</p> <pre><code>pip install sparse_autoencoder\n</code></pre>"},{"location":"#demo","title":"Demo","text":"<p>Check out the demo notebook for a guide to using this library.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>Demo</li> <li>Reference</li> <li>Contributing</li> <li>Citation</li> </ul>"},{"location":"citation/","title":"Citation","text":"<p>Please cite this library as:</p> <pre><code>@misc{cooney2023SparseAutoencoder,\n    title = {Sparse Autoencoder Library},\n    author = {Alan Cooney},\n    year = {2023},\n    howpublished = {\\url{https://github.com/ai-safety-foundation/sparse_autoencoder}},\n}\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#setup","title":"Setup","text":"<p>This project uses Poetry for dependency management, and PoeThePoet for scripts. After checking out the repo, we recommend setting poetry's config to create the <code>.venv</code> in the root directory (note this is a global setting) and then installing with the dev and demos dependencies.</p> <pre><code>poetry config virtualenvs.in-project true\npoetry install --with dev,demos\n</code></pre> <p>If you are using VSCode we highly recommend installing the recommended extensions as well (it will prompt you to do this when you checkout the repo).</p>"},{"location":"contributing/#checks","title":"Checks","text":"<p>For a full list of available commands (e.g. <code>test</code> or <code>typecheck</code>), run this in your terminal (assumes the venv is active already).</p> <pre><code>poe\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Please make sure to add thorough documentation for any features you add. You should do this directly in the docstring, and this will then automatically generate the API docs when merged into <code>main</code>. They will also be automatically checked with pytest (via doctest).</p> <p>If you want to view your documentation changes, run <code>poe docs-hot-reload</code>. This will give you hot-reloading docs (they change in real time as you edit docstrings).</p>"},{"location":"contributing/#docstring-style-guide","title":"Docstring Style Guide","text":"<p>We follow the Google Python Docstring Style for writing docstrings. Some important details below:</p>"},{"location":"contributing/#sections-and-order","title":"Sections and Order","text":"<p>You should follow this order:</p> <pre><code>\"\"\"Title In Title Case.\n\nA description of what the function/class does, including as much detail as is necessary to fully understand it.\n\nWarning:\n\nAny warnings to the user (e.g. common pitfalls).\n\nExamples:\n\nInclude any examples here. They will be checked with doctest.\n\n  &gt;&gt;&gt; print(1 + 2)\n  3\n\nArgs:\n    param_without_type_signature:\n        Each description should be indented once more.\n    param_2:\n        Another example parameter.\n\nReturns:\n    Returns description without type signature.\n\nRaises:\n    Information about the error it may raise (if any).\n\"\"\"\n</code></pre>"},{"location":"contributing/#supported-sphinx-properties","title":"Supported Sphinx Properties","text":""},{"location":"contributing/#references-to-other-functionsclasses","title":"References to Other Functions/Classes","text":"<p>You can reference other parts of the codebase using cross-referencing (noting that you can omit the full path if it is in the same file).</p> <pre><code>:mod:transformer_lens # Function or module\n\n:const:`transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES`\n\n:class:`transformer_lens.HookedTransformer`\n\n:meth:`transformer_lens.HookedTransformer.from_pretrained`\n\n:attr:`transformer_lens.HookedTransformer.cfg`\n</code></pre>"},{"location":"contributing/#maths","title":"Maths","text":"<p>You can use LaTeX, but note that as you're placing this in python strings the backwards slash (<code>\\</code>) must be repeated (i.e. <code>\\\\</code>). You can write LaTeX inline, or in \"display mode\".</p> <pre><code>:math:`(a + b)^2 = a^2 + 2ab + b^2`\n</code></pre> <pre><code>.. math::\n   :nowrap:\n\n   \\\\begin{eqnarray}\n      y    &amp; = &amp; ax^2 + bx + c \\\\\n      f(x) &amp; = &amp; x^2 + 2xy + y^2\n   \\\\end{eqnarray}\n</code></pre>"},{"location":"contributing/#markup","title":"Markup","text":"<ul> <li>Italics - <code>*text*</code></li> <li>Bold - <code>**text**</code></li> <li>Code - <code>``code``</code></li> <li>List items - <code>*item</code></li> <li>Numbered items - <code>1. Item</code></li> <li>Quotes - indent one level</li> <li>External links = <code>`Link text &lt;https://domain.invalid/&gt;`</code></li> </ul>"},{"location":"demo/","title":"Demo","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n\nimport os\nfrom pathlib import Path\n\nimport torch\nfrom transformer_lens import HookedTransformer\nfrom transformer_lens.utils import get_device\nfrom transformers import PreTrainedTokenizerBase\nimport wandb\n\nfrom sparse_autoencoder import SparseAutoencoder\nfrom sparse_autoencoder.activation_resampler import ActivationResampler\nfrom sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss\nfrom sparse_autoencoder.loss.mse_reconstruction_loss import MSEReconstructionLoss\nfrom sparse_autoencoder.loss.reducer import LossReducer\nfrom sparse_autoencoder.optimizer.adam_with_reset import AdamWithReset\nfrom sparse_autoencoder.source_data.text_dataset import GenericTextDataset\nfrom sparse_autoencoder.train.pipeline import Pipeline\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndevice = get_device()\nprint(f\"Using device: {device}\")  # You will need a GPU\n</code></pre> <pre>\n<code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nUsing device: mps\n</code>\n</pre> <p>The way this library works is that you can define your own hyper-parameters and then setup the underlying components with them. This is extremely flexible, but to help you get started we've included some common ones below along with some sensible defaults. You can also easily sweep through multiple hyperparameters with <code>wandb.sweep</code>.</p> <pre><code>hyperparameters = {\n    # Expansion factor is the number of features in the sparse representation, relative to the\n    # number of features in the original MLP layer. The original paper experimented with 1x to 256x,\n    # and we have found that 4x is a good starting point.\n    \"expansion_factor\": 4,\n    # L1 coefficient is the coefficient of the L1 regularization term (used to encourage sparsity).\n    \"l1_coefficient\": 0.001,\n    # Adam parameters (set to the default ones here)\n    \"lr\": 0.001,\n    \"adam_beta_1\": 0.9,\n    \"adam_beta_2\": 0.999,\n    \"adam_epsilon\": 1e-8,\n    \"adam_weight_decay\": 0.0,\n    # Batch sizes\n    \"train_batch_size\": 8192,\n}\n</code></pre> <p>The source model is just a TransformerLens model (see here for a full list of supported models).</p> <p>In this example we're training a sparse autoencoder on the activations from the first MLP layer, so we'll also get some details about that hook point.</p> <pre><code># Source model setup with TransformerLens\nsrc_model_name = \"tiny-stories-1M\"\nsrc_model = HookedTransformer.from_pretrained(src_model_name, dtype=\"float32\")\n\n# Details about the activations we'll train the sparse autoencoder on\nsrc_model_activation_hook_point = \"blocks.0.mlp.hook_post\"\nsrc_model_activation_layer = 0\nsrc_d_mlp: int = src_model.cfg.d_mlp  # type: ignore (TransformerLens typing is currently broken)\n\nf\"Source: {src_model_name}, Hook: {src_model_activation_hook_point}, Features: {src_d_mlp}\"\n</code></pre> <pre>\n<code>Using pad_token, but it is not set yet.\n</code>\n</pre> <pre>\n<code>Loaded pretrained model tiny-stories-1M into HookedTransformer\n</code>\n</pre> <pre>\n<code>'Source: tiny-stories-1M, Hook: blocks.0.mlp.hook_post, Features: 256'</code>\n</pre> <p>We can then setup the sparse autoencoder. The default model (<code>SparseAutoencoder</code>) is setup as per the original Anthropic paper Towards Monosemanticity: Decomposing Language Models With Dictionary Learning .</p> <p>However it's just a standard PyTorch model, so you can create your own model instead if you want to use a different architecture. To do this you just need to extend the <code>AbstractAutoencoder</code>, and optionally the underlying <code>AbstractEncoder</code>, <code>AbstractDecoder</code> and <code>AbstractOuterBias</code>. See these classes (which are fully documented) for more details.</p> <pre><code>expansion_factor = hyperparameters[\"expansion_factor\"]\nautoencoder = SparseAutoencoder(\n    n_input_features=src_d_mlp,  # size of the activations we are autoencoding\n    n_learned_features=int(src_d_mlp * expansion_factor),  # size of SAE\n    geometric_median_dataset=torch.zeros(src_d_mlp),  # this is used to initialize the tied bias\n).to(device)\nautoencoder  # Print the model (it's pretty straightforward)\n</code></pre> <pre>\n<code>SparseAutoencoder(\n  (_pre_encoder_bias): TiedBias(position=pre_encoder)\n  (_encoder): LinearEncoder(\n    in_features=256, out_features=1024\n    (activation_function): ReLU()\n  )\n  (_decoder): UnitNormDecoder(in_features=1024, out_features=256)\n  (_post_decoder_bias): TiedBias(position=post_decoder)\n)</code>\n</pre> <p>We'll also want to setup an Optimizer and Loss function. In this case we'll also use the standard approach from the original Anthropic paper. However you can create your own loss functions and optimizers by extending <code>AbstractLoss</code> and <code>AbstractOptimizerWithReset</code> respectively.</p> <pre><code># We use a loss reducer, which simply adds up the losses from the underlying loss functions.\nloss = LossReducer(\n    LearnedActivationsL1Loss(\n        l1_coefficient=hyperparameters[\"l1_coefficient\"],\n    ),\n    MSEReconstructionLoss(),\n)\nloss\n</code></pre> <pre>\n<code>LossReducer(\n  (0): LearnedActivationsL1Loss(l1_coefficient=0.001)\n  (1): MSEReconstructionLoss()\n)</code>\n</pre> <pre><code>optimizer = AdamWithReset(\n    params=autoencoder.parameters(),\n    named_parameters=autoencoder.named_parameters(),\n    lr=hyperparameters[\"lr\"],\n    betas=(hyperparameters[\"adam_beta_1\"], hyperparameters[\"adam_beta_2\"]),\n    eps=hyperparameters[\"adam_epsilon\"],\n    weight_decay=hyperparameters[\"adam_weight_decay\"],\n)\noptimizer\n</code></pre> <pre>\n<code>AdamWithReset (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0.0\n)</code>\n</pre> <p>Finally we'll initialise an activation resampler.</p> <pre><code>activation_resampler = ActivationResampler()\n</code></pre> <p>This is just a dataset of tokenized prompts, to be used in generating activations (which are in turn used to train the SAE).</p> <pre><code>tokenizer: PreTrainedTokenizerBase = src_model.tokenizer  # type: ignore\nsource_data = GenericTextDataset(tokenizer=tokenizer, dataset_path=\"roneneldan/TinyStories\")\n</code></pre> <pre>\n<code>/Users/alan/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n</code>\n</pre> <p>If you initialise wandb, the pipeline will automatically log all metrics to wandb. However, we should pass in a dictionary with all of our hyperaparameters so they're on  wandb. </p> <p>We strongly encourage users to make use of wandb in order to understand the training process.</p> <pre><code>Path(\".cache/\").mkdir(exist_ok=True)\nwandb.init(\n    project=\"sparse-autoencoder\",\n    dir=\".cache\",\n    config=hyperparameters,\n)\n</code></pre>  Tracking run with wandb version 0.16.0   Run data is saved locally in <code>.cache/wandb/run-20231121_190511-5pvpkttg</code>  Syncing run pious-yogurt-57 to Weights &amp; Biases (docs)   View project at https://wandb.ai/alan-cooney/sparse-autoencoder   View run at https://wandb.ai/alan-cooney/sparse-autoencoder/runs/5pvpkttg Display W&amp;B run <pre><code>pipeline = Pipeline(\n    cache_name=src_model_activation_hook_point,\n    layer=src_model_activation_layer,\n    source_model=src_model,\n    autoencoder=autoencoder,\n    source_dataset=source_data,\n    optimizer=optimizer,\n    loss=loss,\n    activation_resampler=activation_resampler,\n    source_data_batch_size=8,\n)\n\npipeline.run_pipeline(\n    train_batch_size=int(hyperparameters[\"train_batch_size\"]),\n    max_store_size=1_000_000,\n    # Sizes for demo purposes (you probably want to scale these by 10x)\n    max_activations=10_000_000,\n    resample_frequency=2_500_000,\n)\n</code></pre> <pre>\n<code>Activations trained on:   0%|          | 0/10000000 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n---------------------------------------------------------------------------\nRemoteDisconnected                        Traceback (most recent call last)\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\n    789 # Make the request on the HTTPConnection object\n--&gt; 790 response = self._make_request(\n    791     conn,\n    792     method,\n    793     url,\n    794     timeout=timeout_obj,\n    795     body=body,\n    796     headers=headers,\n    797     chunked=chunked,\n    798     retries=retries,\n    799     response_conn=response_conn,\n    800     preload_content=preload_content,\n    801     decode_content=decode_content,\n    802     **response_kw,\n    803 )\n    805 # Everything went great!\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536, in HTTPConnectionPool._make_request(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\n    535 try:\n--&gt; 536     response = conn.getresponse()\n    537 except (BaseSSLError, OSError) as e:\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connection.py:461, in HTTPConnection.getresponse(self)\n    460 # Get the response from http.client.HTTPConnection\n--&gt; 461 httplib_response = super().getresponse()\n    463 try:\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378, in HTTPConnection.getresponse(self)\n   1377 try:\n-&gt; 1378     response.begin()\n   1379 except ConnectionError:\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318, in HTTPResponse.begin(self)\n    317 while True:\n--&gt; 318     version, status, reason = self._read_status()\n    319     if status != CONTINUE:\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:287, in HTTPResponse._read_status(self)\n    284 if not line:\n    285     # Presumably, the server closed the connection before\n    286     # sending a valid response.\n--&gt; 287     raise RemoteDisconnected(\"Remote end closed connection without\"\n    288                              \" response\")\n    289 try:\n\nRemoteDisconnected: Remote end closed connection without response\n\nDuring handling of the above exception, another exception occurred:\n\nProtocolError                             Traceback (most recent call last)\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/requests/adapters.py:486, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)\n    485 try:\n--&gt; 486     resp = conn.urlopen(\n    487         method=request.method,\n    488         url=url,\n    489         body=request.body,\n    490         headers=request.headers,\n    491         redirect=False,\n    492         assert_same_host=False,\n    493         preload_content=False,\n    494         decode_content=False,\n    495         retries=self.max_retries,\n    496         timeout=timeout,\n    497         chunked=chunked,\n    498     )\n    500 except (ProtocolError, OSError) as err:\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:844, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\n    842     new_e = ProtocolError(\"Connection aborted.\", new_e)\n--&gt; 844 retries = retries.increment(\n    845     method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n    846 )\n    847 retries.sleep()\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/util/retry.py:470, in Retry.increment(self, method, url, response, error, _pool, _stacktrace)\n    469 if read is False or method is None or not self._is_method_retryable(method):\n--&gt; 470     raise reraise(type(error), error, _stacktrace)\n    471 elif read is not None:\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/util/util.py:38, in reraise(tp, value, tb)\n     37 if value.__traceback__ is not tb:\n---&gt; 38     raise value.with_traceback(tb)\n     39 raise value\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\n    789 # Make the request on the HTTPConnection object\n--&gt; 790 response = self._make_request(\n    791     conn,\n    792     method,\n    793     url,\n    794     timeout=timeout_obj,\n    795     body=body,\n    796     headers=headers,\n    797     chunked=chunked,\n    798     retries=retries,\n    799     response_conn=response_conn,\n    800     preload_content=preload_content,\n    801     decode_content=decode_content,\n    802     **response_kw,\n    803 )\n    805 # Everything went great!\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536, in HTTPConnectionPool._make_request(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\n    535 try:\n--&gt; 536     response = conn.getresponse()\n    537 except (BaseSSLError, OSError) as e:\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/urllib3/connection.py:461, in HTTPConnection.getresponse(self)\n    460 # Get the response from http.client.HTTPConnection\n--&gt; 461 httplib_response = super().getresponse()\n    463 try:\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378, in HTTPConnection.getresponse(self)\n   1377 try:\n-&gt; 1378     response.begin()\n   1379 except ConnectionError:\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318, in HTTPResponse.begin(self)\n    317 while True:\n--&gt; 318     version, status, reason = self._read_status()\n    319     if status != CONTINUE:\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:287, in HTTPResponse._read_status(self)\n    284 if not line:\n    285     # Presumably, the server closed the connection before\n    286     # sending a valid response.\n--&gt; 287     raise RemoteDisconnected(\"Remote end closed connection without\"\n    288                              \" response\")\n    289 try:\n\nProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionError                           Traceback (most recent call last)\n/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb Cell 26 line 1\n      &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; pipeline = Pipeline(\n      &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt;     cache_name=src_model_activation_hook_point,\n      &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt;     layer=src_model_activation_layer,\n   (...)\n     &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=9'&gt;10&lt;/a&gt;     source_data_batch_size=8,\n     &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt; )\n---&gt; &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=12'&gt;13&lt;/a&gt; pipeline.run_pipeline(\n     &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=13'&gt;14&lt;/a&gt;     train_batch_size=int(hyperparameters[\"train_batch_size\"]),\n     &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=14'&gt;15&lt;/a&gt;     max_store_size=1_000_000,\n     &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=15'&gt;16&lt;/a&gt;     # Sizes for demo purposes (you probably want to scale these by 10x)\n     &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=16'&gt;17&lt;/a&gt;     max_activations=10_000_000,\n     &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=17'&gt;18&lt;/a&gt;     resample_frequency=2_500_000,\n     &lt;a href='vscode-notebook-cell:/Users/alan/Documents/Repos/sparse_autoencoder/demo.ipynb#Y104sZmlsZQ%3D%3D?line=18'&gt;19&lt;/a&gt; )\n\nFile ~/Documents/Repos/sparse_autoencoder/sparse_autoencoder/train/abstract_pipeline.py:186, in AbstractPipeline.run_pipeline(self, train_batch_size, max_store_size, max_activations, resample_frequency, validate_frequency, checkpoint_frequency)\n    183 for _ in range(0, max_activations, store_size):\n    184     # Generate\n    185     progress_bar.set_postfix({\"stage\": \"generate\"})\n--&gt; 186     activation_store: TensorActivationStore = self.generate_activations(store_size)\n    188     # Train\n    189     progress_bar.set_postfix({\"stage\": \"train\"})\n\nFile ~/Documents/Repos/sparse_autoencoder/sparse_autoencoder/train/pipeline.py:45, in Pipeline.generate_activations(self, store_size)\n     43 # Loop through the dataloader until the store reaches the desired size\n     44 with torch.no_grad():\n---&gt; 45     for batch in self.source_data:\n     46         input_ids: BatchTokenizedPrompts = batch[\"input_ids\"].to(source_model_device)\n     47         self.source_model.forward(input_ids, stop_at_layer=self.layer + 1)  # type: ignore (TLens is typed incorrectly)\n\nFile ~/Documents/Repos/sparse_autoencoder/sparse_autoencoder/train/abstract_pipeline.py:262, in AbstractPipeline.stateful_dataloader_iterable(dataloader)\n    227 @staticmethod\n    228 def stateful_dataloader_iterable(\n    229     dataloader: DataLoader[TorchTokenizedPrompts]\n    230 ) -&gt; Iterable[TorchTokenizedPrompts]:\n    231     \"\"\"Create a stateful dataloader iterable.\n    232 \n    233     Create an iterable that maintains it's position in the dataloader between loops.\n   (...)\n    260         Stateful iterable over the data in the dataloader.\n    261     \"\"\"\n--&gt; 262     yield from dataloader\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630, in _BaseDataLoaderIter.__next__(self)\n    627 if self._sampler_iter is None:\n    628     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    629     self._reset()  # type: ignore[call-arg]\n--&gt; 630 data = self._next_data()\n    631 self._num_yielded += 1\n    632 if self._dataset_kind == _DatasetKind.Iterable and \\\n    633         self._IterableDataset_len_called is not None and \\\n    634         self._num_yielded &gt; self._IterableDataset_len_called:\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674, in _SingleProcessDataLoaderIter._next_data(self)\n    672 def _next_data(self):\n    673     index = self._next_index()  # may raise StopIteration\n--&gt; 674     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    675     if self._pin_memory:\n    676         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:32, in _IterableDatasetFetcher.fetch(self, possibly_batched_index)\n     30 for _ in possibly_batched_index:\n     31     try:\n---&gt; 32         data.append(next(self.dataset_iter))\n     33     except StopIteration:\n     34         self.ended = True\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:1379, in IterableDataset.__iter__(self)\n   1376         yield formatter.format_row(pa_table)\n   1377     return\n-&gt; 1379 for key, example in ex_iterable:\n   1380     if self.features:\n   1381         # `IterableDataset` automatically fills missing columns with None.\n   1382         # This is done with `_apply_feature_types_on_example`.\n   1383         example = _apply_feature_types_on_example(\n   1384             example, self.features, token_per_repo_id=self._token_per_repo_id\n   1385         )\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:982, in BufferShuffledExamplesIterable.__iter__(self)\n    980 # this is the shuffle buffer that we keep in memory\n    981 mem_buffer = []\n--&gt; 982 for x in self.ex_iterable:\n    983     if len(mem_buffer) == buffer_size:  # if the buffer is full, pick and example from it\n    984         i = next(indices_iterator)\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:678, in MappedExamplesIterable.__iter__(self)\n    676     yield from ArrowExamplesIterable(self._iter_arrow, {})\n    677 else:\n--&gt; 678     yield from self._iter()\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:693, in MappedExamplesIterable._iter(self)\n    690     format_dict = None\n    692 if self.batched:\n--&gt; 693     for key, example in iterator:\n    694         # If `batched`, first build the batch, if `batch_size` is None or &lt;=0, then the batch is the whole dataset\n    695         iterator_batch = (\n    696             iterator\n    697             if self.batch_size is None or self.batch_size &lt;= 0\n    698             else islice(iterator, self.batch_size - 1)\n    699         )\n    700         key_examples_list = [(key, example)] + list(iterator_batch)\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:1114, in TypedExamplesIterable.__iter__(self)\n   1111 def __iter__(self):\n   1112     # Then for each example, `TypedExamplesIterable` automatically fills missing columns with None.\n   1113     # This is done with `_apply_feature_types_on_example`.\n-&gt; 1114     for key, example in self.ex_iterable:\n   1115         yield key, _apply_feature_types_on_example(\n   1116             example, self.features, token_per_repo_id=self.token_per_repo_id\n   1117         )\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/iterable_dataset.py:320, in ShuffledDataSourcesArrowExamplesIterable.__iter__(self)\n    318 kwargs_with_shuffled_shards = _shuffle_gen_kwargs(rng, self.kwargs)\n    319 formatter = PythonFormatter()\n--&gt; 320 for key, pa_table in self.generate_tables_fn(**kwargs_with_shuffled_shards):\n    321     for pa_subtable in pa_table.to_reader(max_chunksize=config.ARROW_READER_BATCH_SIZE_IN_DATASET_ITER):\n    322         formatted_batch = formatter.format_batch(pa_subtable)\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/packaged_modules/parquet/parquet.py:87, in Parquet._generate_tables(self, files)\n     85 parquet_file = pq.ParquetFile(f)\n     86 try:\n---&gt; 87     for batch_idx, record_batch in enumerate(\n     88         parquet_file.iter_batches(batch_size=self.config.batch_size, columns=self.config.columns)\n     89     ):\n     90         pa_table = pa.Table.from_batches([record_batch])\n     91         # Uncomment for debugging (will print the Arrow table size and elements)\n     92         # logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\n     93         # logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/pyarrow/_parquet.pyx:1366, in iter_batches()\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/pyarrow/types.pxi:88, in pyarrow.lib._datatype_to_pep3118()\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py:333, in _add_retries_to_file_obj_read_method.&lt;locals&gt;.read_with_retries(*args, **kwargs)\n    331 for retry in range(1, max_retries + 1):\n    332     try:\n--&gt; 333         out = read(*args, **kwargs)\n    334         break\n    335     except (ClientError, TimeoutError) as err:\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/fsspec/spec.py:1856, in AbstractBufferedFile.read(self, length)\n   1853 if length == 0:\n   1854     # don't even bother calling fetch\n   1855     return b\"\"\n-&gt; 1856 out = self.cache._fetch(self.loc, self.loc + length)\n   1857 self.loc += len(out)\n   1858 return out\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/fsspec/caching.py:189, in ReadAheadCache._fetch(self, start, end)\n    187     part = b\"\"\n    188 end = min(self.size, end + self.blocksize)\n--&gt; 189 self.cache = self.fetcher(start, end)  # new block replaces old\n    190 self.start = start\n    191 self.end = self.start + len(self.cache)\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:444, in HfFileSystemFile._fetch_range(self, start, end)\n    433 headers = {\n    434     \"range\": f\"bytes={start}-{end - 1}\",\n    435     **self.fs._api._build_hf_headers(),\n    436 }\n    437 url = hf_hub_url(\n    438     repo_id=self.resolved_path.repo_id,\n    439     revision=self.resolved_path.revision,\n   (...)\n    442     endpoint=self.fs.endpoint,\n    443 )\n--&gt; 444 r = http_backoff(\"GET\", url, headers=headers)\n    445 hf_raise_for_status(r)\n    446 return r.content\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:267, in http_backoff(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\n    264     kwargs[\"data\"].seek(io_obj_initial_pos)\n    266 # Perform request and return if status_code is not in the retry list.\n--&gt; 267 response = session.request(method=method, url=url, **kwargs)\n    268 if response.status_code not in retry_on_status_codes:\n    269     return response\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n    584 send_kwargs = {\n    585     \"timeout\": timeout,\n    586     \"allow_redirects\": allow_redirects,\n    587 }\n    588 send_kwargs.update(settings)\n--&gt; 589 resp = self.send(prep, **send_kwargs)\n    591 return resp\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs)\n    700 start = preferred_clock()\n    702 # Send the request\n--&gt; 703 r = adapter.send(request, **kwargs)\n    705 # Total elapsed time of the request (approximately)\n    706 elapsed = preferred_clock() - start\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:63, in UniqueRequestIdAdapter.send(self, request, *args, **kwargs)\n     61 \"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\n     62 try:\n---&gt; 63     return super().send(request, *args, **kwargs)\n     64 except requests.RequestException as e:\n     65     request_id = request.headers.get(X_AMZN_TRACE_ID)\n\nFile ~/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/requests/adapters.py:501, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)\n    486     resp = conn.urlopen(\n    487         method=request.method,\n    488         url=url,\n   (...)\n    497         chunked=chunked,\n    498     )\n    500 except (ProtocolError, OSError) as err:\n--&gt; 501     raise ConnectionError(err, request=request)\n    503 except MaxRetryError as e:\n    504     if isinstance(e.reason, ConnectTimeoutError):\n    505         # TODO: Remove this in 3.0.0: see #2811\n\nConnectionError: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: a88fef33-195d-468c-b527-20ad75eccf2d)')</pre> <pre><code>wandb.finish()\n</code></pre> <pre>\n<code>VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))</code>\n</pre> <pre>\n<code>wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n</code>\n</pre> Run history:LearnedActivationsL1Loss\u2588\u2583\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581LossReducer\u2588\u2583\u2583\u2583\u2582\u2582\u2582\u2582\u2581\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581MSEReconstructionLoss\u2588\u2585\u2585\u2585\u2584\u2583\u2583\u2583\u2582\u2582\u2582\u2582\u2582\u2582\u2581\u2582\u2581\u2581\u2581\u2582\u2581\u2581\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2582\u2581\u2582\u2581\u2582Run summary:LearnedActivationsL1Loss0.00027LossReducer0.01715MSEReconstructionLoss0.01688   View run effortless-donkey-54 at: https://wandb.ai/alan-cooney/sparse-autoencoder/runs/1teoia5bSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)   Find logs at: <code>.cache/wandb/run-20231120_211221-1teoia5b/logs</code> <p> </p>"},{"location":"demo/#sparse-autoencoder-training-demo","title":"Sparse Autoencoder Training Demo","text":"<p>This demo trains a sparse autoencoder on activations from a Tiny Stories 1M model.</p> <p>To do this we setup a source model (the TinyStories model) that we want to generate activations from, along with a source dataset of prompts to help generate these activations.</p> <p>We also setup a sparse autoencoder model which we'll train on these generated activations, to learn a sparse representation of them in higher dimensional space.</p> <p>Finally we'll wrap this all together in a pipeline, which alternates between generating activations (storing them in ram), and training the SAE on said activations.</p>"},{"location":"demo/#setup","title":"Setup","text":""},{"location":"demo/#imports","title":"Imports","text":""},{"location":"demo/#hyperparameters","title":"Hyperparameters","text":""},{"location":"demo/#source-model","title":"Source Model","text":""},{"location":"demo/#sparse-autoencoder","title":"Sparse Autoencoder","text":""},{"location":"demo/#source-dataset","title":"Source dataset","text":""},{"location":"demo/#training","title":"Training","text":""},{"location":"demo/#training-advice","title":"Training Advice","text":"<p>-- Unfinished --</p> <ul> <li>Check recovery loss is low while sparsity is low as well (&lt;20 L1) usually.</li> <li>Can't be sure features are useful until you dig into them more. </li> </ul>"},{"location":"demo/#analysis","title":"Analysis","text":"<p>-- Unfinished --</p>"}]}